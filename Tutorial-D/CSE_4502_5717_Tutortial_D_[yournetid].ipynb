{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2shN--zSfOM"
      },
      "source": [
        "#Extra credit: Building LSTM for training\n",
        "\n",
        "In this extra credit assignment, you will try to build your own LSTM network for application. The steps are following:\n",
        "Loading the package and IMDB data.\n",
        "Build the LSTM network base on description. In this step, you need to add codes to the LSTM building and validate your network. Please follow these steps in order, and add codes to build the model in the corresponding place."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4E0-CbFYI1I"
      },
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "ox9Ka--uYI1Q"
      },
      "source": [
        "from keras.layers import SimpleRNN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg-bh_SOYI1T"
      },
      "source": [
        "#Now let's try to use such a model on the IMDB movie review classification problem. First, let's preprocess the data:\n",
        "\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "max_features = 10000  # number of words to consider as features\n",
        "maxlen = 500  # cut texts after this number of words (among top max_features most common words)\n",
        "batch_size = 32\n",
        "\n",
        "print('Loading data...')\n",
        "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(input_train), 'train sequences')\n",
        "print(len(input_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
        "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
        "print('input_train shape:', input_train.shape)\n",
        "print('input_test shape:', input_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W_7CdcMYI1V"
      },
      "source": [
        "\n",
        "\n",
        "## A challenge of LSTM in Keras\n",
        "\n",
        "Now let's switch to more practical concerns: we will set up a model using a LSTM layer and train it on the IMDB data. Here's the network, \n",
        "similar to the one with `SimpleRNN` that we just presented. We only specify the output dimensionality of the LSTM layer, and leave every \n",
        "other argument (there are lots) to the Keras defaults. Keras has good defaults, and things will almost always \"just work\" without you \n",
        "having to spend time tuning parameters by hand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvqGvjYaYI1V"
      },
      "source": [
        "from keras.layers import LSTM\n",
        "\n",
        "# build your LSTM model and train your model \n",
        "\n",
        "\n",
        "#choose your optimizer and loss function\n",
        "model.compile(optimizer='',\n",
        "              loss='',\n",
        "              metrics=['acc'])\n",
        "\n",
        "#set the number of epochs and batch size\n",
        "history = model.fit(input_train, y_train,\n",
        "                    epochs=,\n",
        "                    batch_size=,\n",
        "                    validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UgC4SNcYI1V"
      },
      "source": [
        "#print the accuracy and loss curves\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}